## 概念
梯度提升是一种用于回归和分类任务等的机器学习技术。它以弱预测模型集合的形式给出了一个预测模型，这些模型通常是决策树。当决策树是弱学习器时，生成的算法称为梯度提升树；它通常优于随机森林。与其他提升方法一样，梯度提升树模型以阶段方式构建，但它通过允许优化任意可微损失函数来推广其他方法。

先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。

**提升树算法:**

(1) 初始化$f_0(x)=0$

(2) 对$m=1,2…M$

(a)计算残差

$$
r_{m_i}=y_{i}-f_{m-1}\left(x_{i}\right), i=1,2, \ldots, M
$$

(b) 拟合残差 $r_{mi}$ 学习一个回归树，得到 $h_m(x)$

(c) 更新 $f_m(x)=f_{m−1}(x)+h_m(x)$

(3)得到回归树

$$f_{M}(x)=\sum_{m=1}^{M} h_{m}(x)$$

上面伪代码中的残差是什么？

在提升树算法中，假设我们前一轮迭代得到的强学习器是

$$f_{t−1}(x)$$

损失函数是

$$L(y,f_{t−1}(x))$$

我们本轮迭代的目标是找到一个弱学习器

$$h_t(x)$$

当采用平方损失函数时
$$
\begin{align}
&L\left(y, f_{t-1}(x)+h_{t}(x)\right)\\
=&\left(y-f_{t-1}(x)-h_{t}(x)\right)^{2}\\
=&\left(r-h_{t}(x)\right)^{2}

\end{align}
$$
这里，

$$r=y−f_{t−1}(x)$$

是当前模型拟合数据的残差（residual）。所以，对于提升树来说只需要简单地拟合当前模型的残差。

回到我们上面讲的那个通俗易懂的例子中，第一次迭代的残差是10岁，第二次残差4岁…

当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Freidman提出了梯度提升树算法，这是利用最速下降的近似方法，**其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。**

那么负梯度长什么样呢？

第t轮的第i个样本的损失函数的负梯度为：

$$-\left[\frac{\partial L\left(y, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{t-1}(x)}$$

此时不同的损失函数将会得到不同的负梯度，如果选择平方损失

$$L\left(y, f\left(x_{i}\right)\right)=\frac{1}{2}\left(y-f\left(x_{i}\right)\right)^{2}$$

负梯度为

$$-\left[\frac{\partial L\left(y, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f t-1(x)}=-\left[\frac{\partial \frac{1}{2}\left(y-f\left(x_{i}\right)\right)^{2}}{\partial f\left(x_{i}\right)}\right]_{f(x)=f t-1(x)}=y-f\left(x_{i}\right)$$

此时我们发现GBDT的**负梯度就是残差**，所以说对于回归问题，我们要拟合的就是残差。  
   
那么对于分类问题呢？二分类和多分类的损失函数都是log loss，**本文以回归问题为例进行讲解。**

