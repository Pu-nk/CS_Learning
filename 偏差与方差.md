## 偏差与方差
一般来说模型的总误差=偏差+方差，训练集上的误差视为是偏差，测试集上相对训练集表现的误差为方差，以下分几种情况来讨论
- **训练集误差15%， 测试集误差16%**
	- 此时训练集误差和测试集误差相似，说明模型的主要误差在于模型学习的偏差较大，说明模型是欠拟合的，应当调整模型的结果提升训练集上的效果
	- 添加层和神经元数量
- **训练集误差5%， 测试集误差16%**
	- 此时模型主要的误差在于测试集和训练集之间的方差，因此应该专注于降低方差的方法
	- 增加训练集的数据量
```ad-note
如果使用了L2正则化或者是Dropout层，那么增加模型规模算法的表现将会保持不变或者提升
```

## 减小可避免偏差的技巧
- 加大模型规模：例如增加神经网络的层或神经元，来使得模型更好的拟合数据集，如果这样做会造成方差的增加，可以加入正则项抵消方差的增加
- 增加额外的特征：增加额外的特征能够帮助算法消除某个类型额外的误差，但是特征数目越多模型的防擦好也会越大，可以通过加入正则项来抵消方差的增加
- 减小或者去除正则项（L2, L1, dropout）
- 修改模型架构：使得网络结构更加适合应用场景
## 减小方差的技巧
- 精简特征：剔除掉对于预测影响很小的特征（可以实现一个batch drop feature算法）
- 增添训练数据
- 加入正则化
- 增加Early Stopping
## 训练集误差分析
- 把训练集中学习效果差的样本找出来分析这些样本的共性
## 在不同的数据分布上测试和训练
- 假设两个数据集$D_1$ 和$D_2$ 数据分布不相同，一个简单的办法是将两个数据整合并随机打乱，但是一个比较重要的原则是选择验证集和测试集作为将来想要正确处理的数据