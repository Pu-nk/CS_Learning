## 概念
决策树学习是一种用于统计、数据挖掘和机器学习的监督学习方法。在这种形式中，分类或回归决策树被用作预测模型，以得出关于一组观察结果的结论。
目标变量可以采用一组离散值的树模型称为分类树；在这些树结构中，叶子代表类标签，分支代表导致这些类标签的特征的结合。目标变量可以取连续值（通常是实数）的决策树称为回归树。
## 方法
对于回归树算法来说最重要的是寻找最佳的划分点，那么回归树中的可划分点包含了所有特征的所有可取的值。在分类树中最佳划分点的判别标准是熵或者基尼系数，都是用纯度来衡量的，但是在回归树中的样本标签是连续数值，所以再使用熵之类的指标不再合适，取而代之的是平方误差，它能很好的评判拟合程度。

**回归树生成算法:**  
输入: 训练数据集 $D$  
输出: 回归树 $f(x)$ 
在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：

(1) 选择最优切分变量 $j$ 与切分点 $s$， 求解:

$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
$$

遍历变量 jj，对固定的切分变量j扫描切分点 ss，选择使得上式达到最小值的对 $(j,s)$.**简要解释一下上述公式**：中括号里面的公式是求出每个特征变量在哪一个划分点时损失函数最小，最外面的 minmin 是在所有特征值，求得使损失函数全局最小的特征及其切分点$(j^_, s^_)$;

(2) 用选定的对 $(j,s)$ 划分区域并决定相应的输出值：

$$
\begin{gathered}
R_{1}(j, s)=\left\{x \mid x^{(j)} \leq s\right\}, R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\} \\
\hat{c_{m}}=\frac{1}{N} \sum_{x 1 \in R_{m}(j, s)} y_{i}, x \in R_{m}, m=1,2
\end{gathered}
$$

求划分区域的输出值就是将该区域的所有样本的输出值求平均。

(3)继续对两个子区域调用步骤（1）和（2），直至满足停止条件。

(4)将输入空间划分为M个区域$R_1,R_2…R_M$，得到决策树

$$f(x)=\sum_{m=1}^{M} \hat{c_{m}} I\left(x \in R_{m}\right)$$